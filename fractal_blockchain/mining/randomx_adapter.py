import hashlib
import os
import random

from fractal_blockchain.core.addressing import AddressedFractalCoordinate, coord_to_string

# Removed local FractalCoordinate class definition.
# All functions will now expect 'fractal_coord: AddressedFractalCoordinate'.

def simulate_randomx_hash(input_data: bytes, fractal_coord: AddressedFractalCoordinate, memory_size_mb: int = 16, iterations: int = 1000) -> str:
    """
    Simulates a memory-hard hashing function like RandomX, incorporating fractal coordinate data.

    Args:
        input_data: The primary data to hash (e.g., block header data).
        fractal_coord: The fractal coordinate associated with this mining attempt.
        memory_size_mb: The size of the memory buffer to use (simulating memory-hardness).
        iterations: The number of iterations to perform (simulating computational work).

    Returns:
        A hex string representing the hash.
    """
    if memory_size_mb <= 0:
        raise ValueError("Memory size must be positive.")
    if iterations <= 0:
        raise ValueError("Iterations must be positive.")

    # 1. Initialize a large memory buffer
    #    In a real RandomX implementation, this would be the "dataset" or "cache"
    buffer_size_bytes = memory_size_mb * 1024 * 1024
    try:
        # Fill with pseudo-random data influenced by input_data and fractal_coord
        # to make the buffer unique to this hashing attempt.
        seed_data = input_data + coord_to_string(fractal_coord).encode('utf-8')
        random.seed(hash(seed_data)) # Seed Python's PRNG

        # In a real scenario, this buffer would be filled with more complex,
        # structured pseudo-random data generated by RandomX's key generation.
        # For simulation, we'll use a simpler approach.
        memory_buffer = bytearray(random.getrandbits(8) for _ in range(buffer_size_bytes))
    except MemoryError:
        raise MemoryError(f"Failed to allocate {memory_size_mb}MB. Reduce memory_size_mb or ensure sufficient system memory.")

    # 2. Combine input_data and fractal_coord data
    combined_input = hashlib.sha256(input_data + coord_to_string(fractal_coord).encode('utf-8')).digest()

    # 3. Perform pseudo-random memory accesses and computations
    #    This simulates RandomX's "program" execution on the dataset.
    current_hash = combined_input
    for i in range(iterations):
        # Use parts of the current_hash to determine memory addresses to read/write.
        # This is a highly simplified version of RandomX's pseudo-random program execution.
        addr1_offset = int.from_bytes(current_hash[:4], 'little') % (buffer_size_bytes - 16) # Ensure we can read 16 bytes
        addr2_offset = int.from_bytes(current_hash[4:8], 'little') % (buffer_size_bytes - 16)

        data_chunk1 = memory_buffer[addr1_offset : addr1_offset + 16]
        data_chunk2 = memory_buffer[addr2_offset : addr2_offset + 16]

        # Modify data in the buffer (simplified)
        modified_chunk = bytes(a ^ b for a, b in zip(data_chunk1, data_chunk2))
        memory_buffer[addr1_offset : addr1_offset + len(modified_chunk)] = modified_chunk

        # Hash the combined_input, data read from memory, and iteration number
        # In real RandomX, operations are more complex (AES rounds, integer math, branches)
        sha_intermediate = hashlib.sha256()
        sha_intermediate.update(current_hash)
        sha_intermediate.update(data_chunk1)
        sha_intermediate.update(data_chunk2)
        sha_intermediate.update(str(i).encode()) # Add iteration to change hash path
        sha_intermediate.update(coord_to_string(fractal_coord).encode('utf-8')) # Reinforce fractal specificity
        current_hash = sha_intermediate.digest()

    # 4. Final hash
    final_hash = hashlib.sha256(current_hash).hexdigest()

    # Clean up large buffer explicitly (though Python's GC would eventually)
    del memory_buffer

    return final_hash


# --- Additional Simulated Algorithm Variants ---

def simulate_algo_variant_A(input_data: bytes, fractal_coord: AddressedFractalCoordinate, fixed_iterations: int = 1500) -> str:
    """
    Variant A: Computationally intensive, less memory-focused than main RandomX sim.
    Uses fractal coordinate depth to slightly alter computational path.
    """
    fractal_coord_bytes = coord_to_string(fractal_coord).encode('utf-8')
    combined_input = hashlib.sha512(input_data + fractal_coord_bytes).digest()
    current_val = int.from_bytes(combined_input[:8], 'big') # Start with a large number

    for i in range(fixed_iterations + fractal_coord.depth * 10): # Computation increases slightly with depth
        # Some arbitrary complex math operations
        # Use first 2 bytes of fractal_coord_bytes, or pad if shorter
        coord_feature_bytes = (fractal_coord_bytes + b'\x00\x00')[:2]
        current_val = (current_val * current_val + i*71 + int.from_bytes(coord_feature_bytes, 'little')) % (2**64)
        if i % 100 == 0: # Hash periodically to mix bits
            current_val_bytes = current_val.to_bytes(8, 'big')
            current_val = int.from_bytes(hashlib.sha256(current_val_bytes).digest()[:8], 'big')

    final_hash = hashlib.sha256(current_val.to_bytes(8, 'big')).hexdigest()
    return final_hash


def simulate_algo_variant_B(input_data: bytes, fractal_coord: AddressedFractalCoordinate, memory_mb: int = 4, num_reads: int = 500) -> str:
    """
    Variant B: Focuses on pseudo-random reads from a smaller, coordinate-seeded memory region.
    Memory pattern varies based on fractal path.
    """
    fractal_coord_bytes = coord_to_string(fractal_coord).encode('utf-8')
    seed_data = input_data + fractal_coord_bytes
    random.seed(hash(seed_data)) # Seed Python's PRNG

    buffer_size_bytes = memory_mb * 1024 * 1024
    try:
        memory_buffer = bytearray(random.getrandbits(8) for _ in range(buffer_size_bytes))
    except MemoryError:
        # Fallback for very low memory environments during simulation
        memory_buffer = bytearray(random.getrandbits(8) for _ in range(1 * 1024 * 1024))
        buffer_size_bytes = 1 * 1024 * 1024
        if buffer_size_bytes == 0 : return hashlib.sha256(b"mem_error_fallback").hexdigest()

    fractal_coord_bytes = coord_to_string(fractal_coord).encode('utf-8') # Already defined above, but ensure it's used here
    current_hash_material = hashlib.sha256(input_data + fractal_coord_bytes).digest()

    path_feature = sum(fractal_coord.path) if fractal_coord.path else 0

    for i in range(num_reads):
        # Address generation influenced by fractal path and current hash material
        addr_offset_seed = int.from_bytes(current_hash_material[:4], 'little') + path_feature + i
        addr_offset = addr_offset_seed % (buffer_size_bytes - 64) # Read 64 bytes

        data_chunk = memory_buffer[addr_offset : addr_offset + 64]

        # Combine and re-hash
        current_hash_material = hashlib.sha256(current_hash_material + data_chunk).digest()

    del memory_buffer
    return hashlib.sha256(current_hash_material).hexdigest()


if __name__ == '__main__':
    # Example Usage with AddressedFractalCoordinate:
    sample_data = b"previous_block_hash_and_other_data"

    # Path elements for AddressedFractalCoordinate are tuples
    coord1 = AddressedFractalCoordinate(depth=5, path=(0, 1, 2, 0, 1))
    coord2 = AddressedFractalCoordinate(depth=5, path=(0, 1, 2, 0, 2)) # Slightly different coord

    print(f"Simulated RandomX-like hash for coord1: {coord_to_string(coord1)}")
    try:
        hash_val1 = simulate_randomx_hash(sample_data, coord1, memory_size_mb=1, iterations=100) # Reduced for quick test
        print(f"Hash 1 (RandomX_Sim): {hash_val1}")

        print(f"\nSimulated Algo_Variant_A hash for coord1: {coord_to_string(coord1)}")
        hash_a1 = simulate_algo_variant_A(sample_data, coord1, fixed_iterations=150)
        print(f"Hash A1 (VariantA): {hash_a1}")

        print(f"\nSimulated Algo_Variant_B hash for coord1: {coord_to_string(coord1)}")
        hash_b1 = simulate_algo_variant_B(sample_data, coord1, memory_mb=1, num_reads=50)
        print(f"Hash B1 (VariantB): {hash_b1}")


        # Demonstrate that changing the fractal coordinate changes the hash for RandomX_Sim
        print(f"\nSimulated RandomX-like hash for coord2: {coord_to_string(coord2)}")
        hash_val2 = simulate_randomx_hash(sample_data, coord2, memory_size_mb=1, iterations=100)
        print(f"Hash 2 (RandomX_Sim): {hash_val2}")
        assert hash_val1 != hash_val2, "RandomX_Sim hashes should differ for different coordinates"

        # Demonstrate that changing the input data changes the hash for RandomX_Sim
        print(f"\nSimulated RandomX-like hash for coord1 with different data:")
        hash_val3 = simulate_randomx_hash(b"other_data", coord1, memory_size_mb=1, iterations=100)
        print(f"Hash 3 (RandomX_Sim): {hash_val3}")
        assert hash_val1 != hash_val3, "RandomX_Sim hashes should differ for different input data"

        print("\nBasic simulation examples seem to work.")

        # Example for larger memory (can be slow)
        # coord_large_mem = AddressedFractalCoordinate(depth=10, path=tuple([0,1]*5))
        # print(f"\nTesting RandomX_Sim with larger memory footprint for {coord_to_string(coord_large_mem)} (e.g., 16MB, 200 iterations):")
        # hash_large_mem = simulate_randomx_hash(sample_data, coord_large_mem, memory_size_mb=16, iterations=200)
        # print(f"Hash (16MB, 200 iter): {hash_large_mem}")

    except MemoryError as e:
        print(f"Memory Error: {e}. Try with a smaller memory_size_mb for testing.")
    except Exception as e:
        print(f"An error occurred: {e}")
